{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOiCB6myLpxgeEzdTlnz9G9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"lyMVKkjGd0PS"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import re, string\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","from nltk.stem import SnowballStemmer\n","from nltk.corpus import wordnet\n","from nltk.stem import WordNet Lemmat izer\n","nltk.download (\"punkt\")\n","nltk.download (\"averaged_perceptron_tagger\")\n","nltk.download (\"wordnet\")\n","nltk.download (\"stopwords\")\n","nltk.download (\"omw-1.4\")\n","from sklearn.model_selection import train_test_split\n","from sklearn. linear_model import LogisticRegression\n","from sklearn. linear_model import SGDClassifier\n","from sklearn. naive_bayes import MultinomialNB\n","from sklearn.metrics import classification_report, f1_score, accuracy_score, confussion_matrix\n","from sklearn.metrics import roc_curve, auc, roc_auc_score\n","from sklearn.featune_extraction. text import TfidfVectorizer\n","from sklearn.featurte_extraction. text import CountVectorizer\n","import gensim\n","from gensim. models import Hord2Vec"]},{"cell_type":"code","source":["df_train = pd.read_csv(\"https://raw.githubusercontent.com/raditya2010631170111/df_20111/main/train.csv\")\n","print(df_train.shape)\n","df_train.head()"],"metadata":{"id":"N31XOVCueWFe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x=df['target'].value_count()\n","print(x)\n","sns.barplot(x.index,x)"],"metadata":{"id":"ZtHlH-Pfekd8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train.isna().sum()"],"metadata":{"id":"UrerEwsKerK-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#1. WoRD-COUNT\n","df_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\n","print(df_train[df_ train['target']==1]['word_count'].mean())\n","print(df_train[df_train['target']==0]['word_count'].mean())"],"metadata":{"id":"AC6dN3Bxezix"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2. CHARACTER-COUNT\n","df_train['char_count'] = df_train['text'].apply(lambda x: len(str(x)))\n","print(df_train[df_train['target']==1]['char_count'].mean())\n","print(df_train[df_train['target']==0]['char_count'].mean())"],"metadata":{"id":"m4lUPshtfFnx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3. UNIQUE WORD-COUNT\n","df_train['unique_word_count'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\n","print(df_train[df_train['target']==1]['unique_word_count'].mean()z)\n","print(df_train[df_train ['target']==0]['unique_word_count'].mean())"],"metadata":{"id":"cUVhvsEofd_A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,4))\n","train_words=df_train[df_train['target']==1]['word_count']\n","ax1.hist(train_words, color='red')\n","ax1.set_title('Disaster tweets'),\n","train_words=df_train[df_train['target']==0]['word_count']\n","ax2.hist(train_words, color='green')\n","ax2.set_title('Non-disaster tweets')\n","fig.suptitle('Words per tweet')\n","plt.show()"],"metadata":{"id":"JIxygghBf1kf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#E-PROCESSING\n","#1. Common text preprocessing\n","text = \" This is a message to be cleaned. It may involve some things 11ke:, ?, :, '' adjacent spaces and tabs\n","def preprocess(text):\n","    text = text.lower()\n","    text=text.strip()\n","    text=re.compile( \" <.*?>\"). sub*, text)\n","    text = re.comp1le('[%s]' % re.escape(string.punctuat1on)).sub(' ', text)\n","    text = re.sub('\\s+', ' ', text)\n","    text = re.sub(r'\\[[0-9]*\\]',' ',text)\n","    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n","    text = re.sub(r'\\d', ' ',text)\n","    text = re.sub(r'\\s+', ' ',text)\n","    return text"],"metadata":{"id":"KxSFcntPgZcT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#1. STOPWORD REMOVAL\n","def stopword (string):\n","a- [i for i in string.split () if i not in stopwords . words (\"english )]\n","return 'join(a)\n","text-stopword (textt)\n","print (text)\n","#2. STEMMING\n","snow Snowbal 1Stemmer ( \"english')\n","def stemming (sdoing):\n","a-[ snow. stem(i) for i in word_tokenize (string) ]\n","return \"\".join(a)\n","text-stemming (text)\n","print (text)\n","#3. LEMMATIZATION\n","wl WordNet Lemmatizer ()\n","def get_wordnet_pos (tag) :\n","if tag.startswith ( \"J'):\n","return wordnet .ADJ\n","elif tap. start suith(\"V):\n","return wordnet. VERB\n","elif tag.startswith (°N°):\n","return wordnet . NOUN\n","elif tag.startswith (\"R°):\n","return wordnet . ADVV\n","else:\n","return wordnet.NOUN\n","#Tokenize thesentence\n","def lemmatizer(string):\n","word_pos_tags nltk. pos_tag(word_tokeni ze(string))\n","a-[wl.lemmatize (tag[®]. get_wordnet _pos (tag[1])) for idx, tag in enumerate (\n","return \" \"-join (a)\n","text lemmatizer (text)\n","print (text)"],"metadata":{"id":"a63T8OA4hfd2"},"execution_count":null,"outputs":[]}]}